{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# coderbyte\n",
    "# !pip install langchain openai\n",
    "# !pip install langchain_community\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef classify_number(x):\\n    match x:\\n        case _ if x < 0:\\n            return \"Negative\"\\n        case _ if x == 0:\\n            return \"Zero\"\\n        case _ if x > 0:\\n            return \"Positive\"\\n\\n# Example usage\\nprint(classify_number(-10))  # Output: Negative\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def classify_number(x):\n",
    "    match x:\n",
    "        case _ if x < 0:\n",
    "            return \"Negative\"\n",
    "        case _ if x == 0:\n",
    "            return \"Zero\"\n",
    "        case _ if x > 0:\n",
    "            return \"Positive\"\n",
    "\n",
    "# Example usage\n",
    "print(classify_number(-10))  # Output: Negative\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Using logging                                                  #\n",
    "# To use logging you need to start it and after that shutdown it #\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename=r'C:\\Users\\PC\\Desktop\\meta_ai\\log\\example.log',  # Corrected the filename\n",
    "    level=logging.DEBUG,  # Ensure all levels are captured\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Log script start\n",
    "logging.info(\"Script started\")\n",
    "\n",
    "try:\n",
    "    # Code that might raise an exception\n",
    "    result = 10 / 2  # Changed to avoid zero division (for demonstration)\n",
    "    logging.info('Computation successful')\n",
    "except Exception as e:\n",
    "    # Log the exception\n",
    "    logging.error(f\"Error occurred: {e}\")\n",
    "finally:\n",
    "    # Log script end\n",
    "    logging.info(\"Script finished\")\n",
    "\n",
    "logging.shutdown()\n",
    "\n",
    "#os.remove(r'C:\\Users\\PC\\Desktop\\meta_ai\\log\\example.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "logging.shutdown()\n",
    "os.remove(r'C:\\Users\\PC\\Desktop\\meta_ai\\log\\example.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Prompt example             #\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age         City\n",
      "0    Alice   25     New York\n",
      "1      Bob   30  Los Angeles\n",
      "2  Charlie   35      Chicago\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame using a dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the environment variables\n",
    "DATABRICKS_TOKEN = os.getenv('TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This appears to be a pandas DataFrame, which is a 2-dimensional labeled data structure with columns of potentially different types. Here's a breakdown of the DataFrame:\n",
      "\n",
      "**Columns:**\n",
      "\n",
      "1. **Name**: This column contains strings representing names of individuals (e.g., \"Alice\", \"Bob\", \"Charlie\").\n",
      "2. **Age**: This column contains integers representing the ages of the individuals (e.g., 25, 30, 35).\n",
      "3. **City**: This column contains strings representing the cities where the individuals are from (e.g., \"New York\", \"Los Angeles\", \"Chicago\").\n",
      "\n",
      "**Rows:**\n",
      "\n",
      "There are 3 rows in this DataFrame, each representing a single individual. The rows are indexed from 0 to 2.\n",
      "\n",
      "**Index:**\n",
      "\n",
      "The index is the column on the left, which is automatically generated by pandas when creating a DataFrame. In this case, the index is a simple integer index (0, 1, 2).\n",
      "\n",
      "Some potential observations and insights from this data:\n",
      "\n",
      "* The ages range from 25 to 35, indicating a relatively young population.\n",
      "* The cities are all major metropolitan areas in the United States, suggesting a possible urban focus.\n",
      "* There is no obvious correlation between age and city, but with only three\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# How to get your Databricks token: https://docs.databricks.com/en/dev-tools/auth/pat.html\n",
    "#DATABRICKS_TOKEN = os.environ.get('DATABRICKS_TOKEN')\n",
    "DATABRICKS_TOKEN\n",
    "#DATABRICKS_TOKEN = '<TOKEN>'\n",
    "# Alternatively in a Databricks notebook you can use this:\n",
    "# DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=DATABRICKS_TOKEN,\n",
    "  base_url=\"https://adb-6523536500265509.9.azuredatabricks.net/serving-endpoints\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "  messages=[\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are an AI assistant\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"Tell me about this dataframne {df}\"\n",
    "  }\n",
    "  ],\n",
    "  model=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "  max_tokens=256\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n",
    "\n",
    "# PermissionDeniedError: Error code: 403 - {'error_code': 'PERMISSION_DENIED', 'message': 'PERMISSION_DENIED: The endpoint is disabled due to a rate limit set to 0.'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Configurando o cliente OpenAI com o token do Databricks\n",
    "databricks_token = '<TOKEN>'\n",
    "openai_api_key = databricks_token\n",
    "\n",
    "# Configurando o LLM para interagir com o modelo Meta LLaMA 3 70B\n",
    "llm = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    model=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    base_url=\"https://adb-6523536500265509.9.azuredatabricks.net/serving-endpoints\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age         City\n",
      "0    Alice   25     New York\n",
      "1      Bob   30  Los Angeles\n",
      "2  Charlie   35      Chicago\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame using a dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um template de prompt\n",
    "template = \"\"\"\n",
    "analisando daaframe:\n",
    "{dataframe}\n",
    "\n",
    "respondendo:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"dataframe\", \"question\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqui vai uma análise descritiva do DataFrame:\n",
      "\n",
      "**Visão Geral**\n",
      "\n",
      "O DataFrame tem 3 linhas e 3 colunas, contendo informações sobre 3 indivíduos.\n",
      "\n",
      "**Análise das Colunas**\n",
      "\n",
      "* **Name**: Essa coluna contém os nomes dos indivíduos. Não há valores ausentes ou inconsistentes.\n",
      "* **Age**: Essa coluna contém a idade dos indivíduos. Os valores variam de 25 a 35 anos.\n",
      "* **City**: Essa coluna contém as cidades onde os indivíduos residem. Não há valores ausentes ou inconsistentes.\n",
      "\n",
      "**Estatísticas Descritivas**\n",
      "\n",
      "* **Média da Idade**: 30 anos\n",
      "* **Moda da Idade**: Não há moda, pois cada idade aparece apenas uma vez.\n",
      "* **Mediana da Idade**: 30 anos\n",
      "* **Desvio Padrão da Idade**: 5 anos\n",
      "\n",
      "**Distribuição da Idade**\n",
      "\n",
      "* A idade mínima é 25 anos (Alice)\n",
      "* A idade máxima é 35 anos (Charlie)\n",
      "* A idade média é 30 anos (Bob)\n",
      "\n",
      "**Distribuição\n"
     ]
    }
   ],
   "source": [
    "# Criando uma cadeia simples\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Um exemplo de dataframe e pergunta\n",
    "dataframe = df\n",
    "\n",
    "question = \"faz uma analise descritiva sobre esse dataframe\"\n",
    "\n",
    "# Executando a cadeia\n",
    "response = chain.run({\"dataframe\": dataframe, \"question\": question})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Databricks, OpenAI, Prompt #\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Configurando o cliente OpenAI com o token do Databricks\n",
    "databricks_token = '<TOKEN>'\n",
    "openai_api_key = databricks_token\n",
    "\n",
    "# Configurando o LLM para interagir com o modelo Meta LLaMA 3 70B\n",
    "llm = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    model=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    base_url=\"https://adb-6523536500265509.9.azuredatabricks.net/serving-endpoints\"\n",
    ")\n",
    "\n",
    "# Criando um template de prompt\n",
    "template = \"\"\"\n",
    "You are an AI assistant that analyzes dataframes. Here is the dataframe:\n",
    "{dataframe}\n",
    "\n",
    "Answer the following question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"dataframe\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Criando uma cadeia simples\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Um exemplo de dataframe e pergunta\n",
    "dataframe = \"\"\"\n",
    "| Name  | Age | Salary |\n",
    "|-------|-----|--------|\n",
    "| Alice | 30  | 70000  |\n",
    "| Bob   | 35  | 80000  |\n",
    "| Carol | 40  | 90000  |\n",
    "\"\"\"\n",
    "\n",
    "question = \"What is the average salary?\"\n",
    "\n",
    "# Executando a cadeia\n",
    "response = chain.run({\"dataframe\": dataframe, \"question\": question})\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "great_expectations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
